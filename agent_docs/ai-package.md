# AI Package (`@ellie/ai`)

Thin wrapper around TanStack AI with a model registry, cost calculation, thinking support, and provider abstractions.

## Subpath Exports

| Import | What it provides |
|--------|-----------------|
| `@ellie/ai` | Everything below + TanStack AI re-exports (`chat`, `summarize`, stream utils, tool defs, message converters) |
| `@ellie/ai/models` | Model registry: `getModel()`, `getModels()`, `getProviders()`, `findModel()`, `modelsAreEqual()`, `MODELS` |
| `@ellie/ai/usage` | Cost calculation: `calculateCost()`, `createUsage()`, `mapTanStackUsage()` |
| `@ellie/ai/overflow` | Context overflow detection: `isContextOverflow()`, `getOverflowPatterns()` |
| `@ellie/ai/env` | API key resolution: `getEnvApiKey()`, `hasEnvApiKey()` |
| `@ellie/ai/thinking` | Thinking support: `toThinkingModelOptions()`, `supportsThinking()` |

## Providers

```ts
type ProviderName = "anthropic" | "openai" | "ollama" | "openrouter"
```

## Model Registry

**Source**: `src/models.generated.ts` — auto-generated by `scripts/generate-models.ts`.

**Data sources**: models.dev API (Anthropic + OpenAI, takes priority) and OpenRouter API. Manual overrides for specific models (cache pricing fixes, context window corrections, manually-added models).

**Registry**: Built as `Map<ProviderName, Map<string, Model>>` at module load for O(1) lookups.

### Model Shape

```ts
interface Model {
  id: string                   // e.g., "claude-opus-4-6"
  name: string                 // e.g., "Claude Opus 4.6"
  provider: ProviderName
  reasoning: boolean           // supports extended thinking
  input: ("text" | "image")[]
  cost: { input, output, cacheRead, cacheWrite }  // $/million tokens
  contextWindow: number        // max input tokens
  maxTokens: number            // max output tokens
}
```

### Lookup Functions

- `getModel(provider, modelId)` — O(1) by provider + ID
- `findModel(modelId)` — linear search across all providers
- `getModels(provider)` — all models for a provider
- `getProviders()` — list of registered providers

### Regenerating Models

```sh
bun run --filter=@ellie/ai generate-models
```

Fetches latest data from APIs, applies overrides, writes `src/models.generated.ts`.

## Cost Calculation

All costs are in **$/million tokens**.

- `calculateCost(model, {input, output, cacheRead?, cacheWrite?})` → `CostBreakdown` with per-type costs + total
- `createUsage(model, tokens)` → full `Usage` object (tokens + costs)
- `mapTanStackUsage(model, tanstackUsage)` → converts TanStack AI's `{promptTokens, completionTokens}` to Ellie's `Usage` type

## Thinking Support

Unified thinking levels across providers:

```ts
type ThinkingLevel = "minimal" | "low" | "medium" | "high" | "xhigh"
```

| Provider | How it maps |
|----------|-------------|
| Anthropic | `thinking.budget_tokens` (1024 → 16384) |
| OpenAI | `reasoning.effort` ("minimal" → "high", xhigh caps at "high") |
| OpenRouter | Same as OpenAI (passthrough) |
| Ollama | No-op (empty object) |

**Usage**: spread into TanStack AI's `chat()` modelOptions:
```ts
chat({
  adapter,
  messages,
  modelOptions: { ...toThinkingModelOptions("anthropic", "high") },
})
```

- `supportsThinking(provider)` — returns true for anthropic, openai, openrouter

## Context Overflow Detection

Two-layer detection:

1. **Pattern matching** — tests error message against provider-specific patterns (Anthropic, OpenAI, OpenRouter, llama.cpp, generic)
2. **Silent detection** — if `inputTokens > contextWindow`, returns true even without an error message

```ts
isContextOverflow(errorMessage, inputTokens?, contextWindow?) → boolean
```

## Environment API Key Resolution

Maps providers to env vars via `@ellie/env/server`:

| Provider | Env var |
|----------|---------|
| anthropic | `ANTHROPIC_API_KEY` |
| openai | `OPENAI_API_KEY` |
| openrouter | `OPENROUTER_API_KEY` |
| ollama | none (no key needed) |

- `getEnvApiKey(provider)` → key string or undefined
- `hasEnvApiKey(provider)` → boolean (always true for ollama)

## TanStack AI Re-exports

The main `@ellie/ai` entry re-exports 20+ functions from TanStack AI:

- **Chat**: `chat()`, `summarize()`, `createChatOptions()`
- **Streaming**: `streamToText()`, `toServerSentEventsStream()`, `toHttpStream()`
- **Tools**: `toolDefinition()`, `ToolCallManager`
- **Agent strategies**: `maxIterations()`, `untilFinishReason()`, `combineStrategies()`
- **Adapters**: `createModel()`, `extendAdapter()`
- **Messages**: `convertMessagesToModelMessages()`, `generateMessageId()`, `uiMessageToModelMessages()`, etc.
- **Processing**: `StreamProcessor`, `createReplayStream()`

## Key Files

| What | Where |
|------|-------|
| Types | `packages/ai/src/types.ts` |
| Model registry | `packages/ai/src/models/index.ts` |
| Generated models | `packages/ai/src/models.generated.ts` |
| Cost calculation | `packages/ai/src/usage.ts` |
| Thinking support | `packages/ai/src/thinking.ts` |
| Overflow detection | `packages/ai/src/overflow.ts` |
| Env key resolution | `packages/ai/src/env.ts` |
| Generator script | `packages/ai/scripts/generate-models.ts` |
| Main exports | `packages/ai/src/index.ts` |
